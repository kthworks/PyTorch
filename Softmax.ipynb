{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prompt-newfoundland",
   "metadata": {},
   "source": [
    "## Softmax regression cost function 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informative-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cea9a319f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-collect",
   "metadata": {},
   "source": [
    "### 1. 파이토치로 소프트맥스 비용함수 구현 ( Low - level )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "controversial-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([1,2,3]) \n",
    "\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)\n",
    "hypothesis.sum() # 합이 1인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "structural-passing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 2, 1])\n",
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 비용함수 직접 구현\n",
    "\n",
    "z = torch.rand(3,5,requires_grad=True) #3x5 random data\n",
    "\n",
    "hypothesis = F.softmax(z, dim=1) #합이 1이되는 vector들로 변경\n",
    "print(hypothesis)\n",
    "\n",
    "#임의의 실제값 만들기\n",
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)\n",
    "\n",
    "#one-hot encoding\n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1),1) #연산뒤에 _를 붙이면 덮어쓰기가 됨 \n",
    "\n",
    "#Softmax 함수 구현\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-registrar",
   "metadata": {},
   "source": [
    "### 파이토치로 소프트맥수의 비용함수 구현하기 ( High - level )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cheap-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#low-level\n",
    "torch.log(F.softmax(z, dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "organized-tuesday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High-level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "imperial-typing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Low-level\n",
    "(y_one_hot*-torch.log(F.softmax(z,dim=1))).sum(dim=1).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lightweight-cookbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#High-level\n",
    "(y_one_hot*-F.log_softmax(z,dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bulgarian-reader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#더 간단히\n",
    "F.nll_loss(F.log_softmax(z,dim=1),y) \n",
    "\n",
    "#F.nll_loss()는 Negative Log Likelihood의 약자로, F.log_softmax()를 수행한 후 남은 수식들을 수행함\n",
    "#F.cross_entropy()는 F.log_softmax()와 F.nll_loss()를 포함하고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "representative-omega",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#제일 간단히\n",
    "F.cross_entropy(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-twins",
   "metadata": {},
   "source": [
    "## Softmax 회귀 구현하기\n",
    "\n",
    "소프트맥스 회귀를 low level과 F.cross_entropy를 사용해서 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "handed-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-christian",
   "metadata": {},
   "source": [
    "### Low level 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "engaged-decline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n",
      "Epoch: 0/2000 Cost: 2.9296\n",
      "Epoch: 100/2000 Cost: 6.0195\n",
      "Epoch: 200/2000 Cost: 5.6420\n",
      "Epoch: 300/2000 Cost: 5.3287\n",
      "Epoch: 400/2000 Cost: 5.0231\n",
      "Epoch: 500/2000 Cost: 4.7239\n",
      "Epoch: 600/2000 Cost: 4.4342\n",
      "Epoch: 700/2000 Cost: 4.1555\n",
      "Epoch: 800/2000 Cost: 3.8880\n",
      "Epoch: 900/2000 Cost: 3.6309\n",
      "Epoch: 1000/2000 Cost: 3.3833\n",
      "Epoch: 1100/2000 Cost: 3.1438\n",
      "Epoch: 1200/2000 Cost: 2.9103\n",
      "Epoch: 1300/2000 Cost: 2.6809\n",
      "Epoch: 1400/2000 Cost: 2.4345\n",
      "Epoch: 1500/2000 Cost: 1.8837\n",
      "Epoch: 1600/2000 Cost: 1.2294\n",
      "Epoch: 1700/2000 Cost: 0.2150\n",
      "Epoch: 1800/2000 Cost: 0.2075\n",
      "Epoch: 1900/2000 Cost: 0.2006\n",
      "Epoch: 2000/2000 Cost: 0.1942\n"
     ]
    }
   ],
   "source": [
    "# set Weight and get z\n",
    "W = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "z = x_train.matmul(W)+b \n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD([W, b], lr = 0.1)\n",
    "\n",
    "# one-hot encoding\n",
    "y_one_hot = torch.zeros_like(z)\n",
    "y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "print(y_one_hot)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    #H(X)\n",
    "    z = x_train.matmul(W)+b\n",
    "    \n",
    "    #Cost function\n",
    "    cost = (y_one_hot*-torch.log(F.softmax(z, dim=1))).sum(dim=0).mean()\n",
    "    \n",
    "    #Update\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: {}/{} Cost: {:.4f}'.format(epoch,nb_epochs,cost.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-scholar",
   "metadata": {},
   "source": [
    "### High level 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "recovered-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2000 Cost: 1.6168\n",
      "Epoch: 10/2000 Cost: 1.4127\n",
      "Epoch: 20/2000 Cost: 1.2778\n",
      "Epoch: 30/2000 Cost: 1.1718\n",
      "Epoch: 40/2000 Cost: 1.0892\n",
      "Epoch: 50/2000 Cost: 1.0252\n",
      "Epoch: 60/2000 Cost: 0.9759\n",
      "Epoch: 70/2000 Cost: 0.9375\n",
      "Epoch: 80/2000 Cost: 0.9073\n",
      "Epoch: 90/2000 Cost: 0.8830\n",
      "Epoch: 100/2000 Cost: 0.8631\n",
      "Epoch: 110/2000 Cost: 0.8465\n",
      "Epoch: 120/2000 Cost: 0.8323\n",
      "Epoch: 130/2000 Cost: 0.8199\n",
      "Epoch: 140/2000 Cost: 0.8090\n",
      "Epoch: 150/2000 Cost: 0.7991\n",
      "Epoch: 160/2000 Cost: 0.7902\n",
      "Epoch: 170/2000 Cost: 0.7819\n",
      "Epoch: 180/2000 Cost: 0.7743\n",
      "Epoch: 190/2000 Cost: 0.7671\n",
      "Epoch: 200/2000 Cost: 0.7603\n",
      "Epoch: 210/2000 Cost: 0.7540\n",
      "Epoch: 220/2000 Cost: 0.7479\n",
      "Epoch: 230/2000 Cost: 0.7421\n",
      "Epoch: 240/2000 Cost: 0.7366\n",
      "Epoch: 250/2000 Cost: 0.7312\n",
      "Epoch: 260/2000 Cost: 0.7261\n",
      "Epoch: 270/2000 Cost: 0.7212\n",
      "Epoch: 280/2000 Cost: 0.7164\n",
      "Epoch: 290/2000 Cost: 0.7118\n",
      "Epoch: 300/2000 Cost: 0.7074\n",
      "Epoch: 310/2000 Cost: 0.7030\n",
      "Epoch: 320/2000 Cost: 0.6988\n",
      "Epoch: 330/2000 Cost: 0.6947\n",
      "Epoch: 340/2000 Cost: 0.6908\n",
      "Epoch: 350/2000 Cost: 0.6869\n",
      "Epoch: 360/2000 Cost: 0.6832\n",
      "Epoch: 370/2000 Cost: 0.6795\n",
      "Epoch: 380/2000 Cost: 0.6759\n",
      "Epoch: 390/2000 Cost: 0.6725\n",
      "Epoch: 400/2000 Cost: 0.6691\n",
      "Epoch: 410/2000 Cost: 0.6657\n",
      "Epoch: 420/2000 Cost: 0.6625\n",
      "Epoch: 430/2000 Cost: 0.6593\n",
      "Epoch: 440/2000 Cost: 0.6562\n",
      "Epoch: 450/2000 Cost: 0.6532\n",
      "Epoch: 460/2000 Cost: 0.6503\n",
      "Epoch: 470/2000 Cost: 0.6474\n",
      "Epoch: 480/2000 Cost: 0.6445\n",
      "Epoch: 490/2000 Cost: 0.6418\n",
      "Epoch: 500/2000 Cost: 0.6390\n",
      "Epoch: 510/2000 Cost: 0.6364\n",
      "Epoch: 520/2000 Cost: 0.6338\n",
      "Epoch: 530/2000 Cost: 0.6312\n",
      "Epoch: 540/2000 Cost: 0.6287\n",
      "Epoch: 550/2000 Cost: 0.6262\n",
      "Epoch: 560/2000 Cost: 0.6238\n",
      "Epoch: 570/2000 Cost: 0.6215\n",
      "Epoch: 580/2000 Cost: 0.6191\n",
      "Epoch: 590/2000 Cost: 0.6169\n",
      "Epoch: 600/2000 Cost: 0.6146\n",
      "Epoch: 610/2000 Cost: 0.6124\n",
      "Epoch: 620/2000 Cost: 0.6103\n",
      "Epoch: 630/2000 Cost: 0.6081\n",
      "Epoch: 640/2000 Cost: 0.6060\n",
      "Epoch: 650/2000 Cost: 0.6040\n",
      "Epoch: 660/2000 Cost: 0.6020\n",
      "Epoch: 670/2000 Cost: 0.6000\n",
      "Epoch: 680/2000 Cost: 0.5980\n",
      "Epoch: 690/2000 Cost: 0.5961\n",
      "Epoch: 700/2000 Cost: 0.5942\n",
      "Epoch: 710/2000 Cost: 0.5924\n",
      "Epoch: 720/2000 Cost: 0.5905\n",
      "Epoch: 730/2000 Cost: 0.5887\n",
      "Epoch: 740/2000 Cost: 0.5869\n",
      "Epoch: 750/2000 Cost: 0.5852\n",
      "Epoch: 760/2000 Cost: 0.5835\n",
      "Epoch: 770/2000 Cost: 0.5818\n",
      "Epoch: 780/2000 Cost: 0.5801\n",
      "Epoch: 790/2000 Cost: 0.5785\n",
      "Epoch: 800/2000 Cost: 0.5768\n",
      "Epoch: 810/2000 Cost: 0.5752\n",
      "Epoch: 820/2000 Cost: 0.5736\n",
      "Epoch: 830/2000 Cost: 0.5721\n",
      "Epoch: 840/2000 Cost: 0.5705\n",
      "Epoch: 850/2000 Cost: 0.5690\n",
      "Epoch: 860/2000 Cost: 0.5675\n",
      "Epoch: 870/2000 Cost: 0.5661\n",
      "Epoch: 880/2000 Cost: 0.5646\n",
      "Epoch: 890/2000 Cost: 0.5632\n",
      "Epoch: 900/2000 Cost: 0.5617\n",
      "Epoch: 910/2000 Cost: 0.5603\n",
      "Epoch: 920/2000 Cost: 0.5590\n",
      "Epoch: 930/2000 Cost: 0.5576\n",
      "Epoch: 940/2000 Cost: 0.5562\n",
      "Epoch: 950/2000 Cost: 0.5549\n",
      "Epoch: 960/2000 Cost: 0.5536\n",
      "Epoch: 970/2000 Cost: 0.5523\n",
      "Epoch: 980/2000 Cost: 0.5510\n",
      "Epoch: 990/2000 Cost: 0.5497\n",
      "Epoch: 1000/2000 Cost: 0.5485\n",
      "Epoch: 1010/2000 Cost: 0.5472\n",
      "Epoch: 1020/2000 Cost: 0.5460\n",
      "Epoch: 1030/2000 Cost: 0.5448\n",
      "Epoch: 1040/2000 Cost: 0.5436\n",
      "Epoch: 1050/2000 Cost: 0.5424\n",
      "Epoch: 1060/2000 Cost: 0.5412\n",
      "Epoch: 1070/2000 Cost: 0.5400\n",
      "Epoch: 1080/2000 Cost: 0.5389\n",
      "Epoch: 1090/2000 Cost: 0.5377\n",
      "Epoch: 1100/2000 Cost: 0.5366\n",
      "Epoch: 1110/2000 Cost: 0.5355\n",
      "Epoch: 1120/2000 Cost: 0.5344\n",
      "Epoch: 1130/2000 Cost: 0.5333\n",
      "Epoch: 1140/2000 Cost: 0.5322\n",
      "Epoch: 1150/2000 Cost: 0.5311\n",
      "Epoch: 1160/2000 Cost: 0.5301\n",
      "Epoch: 1170/2000 Cost: 0.5290\n",
      "Epoch: 1180/2000 Cost: 0.5280\n",
      "Epoch: 1190/2000 Cost: 0.5270\n",
      "Epoch: 1200/2000 Cost: 0.5259\n",
      "Epoch: 1210/2000 Cost: 0.5249\n",
      "Epoch: 1220/2000 Cost: 0.5239\n",
      "Epoch: 1230/2000 Cost: 0.5229\n",
      "Epoch: 1240/2000 Cost: 0.5219\n",
      "Epoch: 1250/2000 Cost: 0.5210\n",
      "Epoch: 1260/2000 Cost: 0.5200\n",
      "Epoch: 1270/2000 Cost: 0.5190\n",
      "Epoch: 1280/2000 Cost: 0.5181\n",
      "Epoch: 1290/2000 Cost: 0.5171\n",
      "Epoch: 1300/2000 Cost: 0.5162\n",
      "Epoch: 1310/2000 Cost: 0.5153\n",
      "Epoch: 1320/2000 Cost: 0.5144\n",
      "Epoch: 1330/2000 Cost: 0.5135\n",
      "Epoch: 1340/2000 Cost: 0.5125\n",
      "Epoch: 1350/2000 Cost: 0.5117\n",
      "Epoch: 1360/2000 Cost: 0.5108\n",
      "Epoch: 1370/2000 Cost: 0.5099\n",
      "Epoch: 1380/2000 Cost: 0.5090\n",
      "Epoch: 1390/2000 Cost: 0.5081\n",
      "Epoch: 1400/2000 Cost: 0.5073\n",
      "Epoch: 1410/2000 Cost: 0.5064\n",
      "Epoch: 1420/2000 Cost: 0.5056\n",
      "Epoch: 1430/2000 Cost: 0.5047\n",
      "Epoch: 1440/2000 Cost: 0.5039\n",
      "Epoch: 1450/2000 Cost: 0.5031\n",
      "Epoch: 1460/2000 Cost: 0.5023\n",
      "Epoch: 1470/2000 Cost: 0.5014\n",
      "Epoch: 1480/2000 Cost: 0.5006\n",
      "Epoch: 1490/2000 Cost: 0.4998\n",
      "Epoch: 1500/2000 Cost: 0.4990\n",
      "Epoch: 1510/2000 Cost: 0.4982\n",
      "Epoch: 1520/2000 Cost: 0.4975\n",
      "Epoch: 1530/2000 Cost: 0.4967\n",
      "Epoch: 1540/2000 Cost: 0.4959\n",
      "Epoch: 1550/2000 Cost: 0.4951\n",
      "Epoch: 1560/2000 Cost: 0.4944\n",
      "Epoch: 1570/2000 Cost: 0.4936\n",
      "Epoch: 1580/2000 Cost: 0.4929\n",
      "Epoch: 1590/2000 Cost: 0.4921\n",
      "Epoch: 1600/2000 Cost: 0.4914\n",
      "Epoch: 1610/2000 Cost: 0.4906\n",
      "Epoch: 1620/2000 Cost: 0.4899\n",
      "Epoch: 1630/2000 Cost: 0.4892\n",
      "Epoch: 1640/2000 Cost: 0.4884\n",
      "Epoch: 1650/2000 Cost: 0.4877\n",
      "Epoch: 1660/2000 Cost: 0.4870\n",
      "Epoch: 1670/2000 Cost: 0.4863\n",
      "Epoch: 1680/2000 Cost: 0.4856\n",
      "Epoch: 1690/2000 Cost: 0.4849\n",
      "Epoch: 1700/2000 Cost: 0.4842\n",
      "Epoch: 1710/2000 Cost: 0.4835\n",
      "Epoch: 1720/2000 Cost: 0.4828\n",
      "Epoch: 1730/2000 Cost: 0.4821\n",
      "Epoch: 1740/2000 Cost: 0.4814\n",
      "Epoch: 1750/2000 Cost: 0.4807\n",
      "Epoch: 1760/2000 Cost: 0.4801\n",
      "Epoch: 1770/2000 Cost: 0.4794\n",
      "Epoch: 1780/2000 Cost: 0.4787\n",
      "Epoch: 1790/2000 Cost: 0.4781\n",
      "Epoch: 1800/2000 Cost: 0.4774\n",
      "Epoch: 1810/2000 Cost: 0.4768\n",
      "Epoch: 1820/2000 Cost: 0.4761\n",
      "Epoch: 1830/2000 Cost: 0.4755\n",
      "Epoch: 1840/2000 Cost: 0.4748\n",
      "Epoch: 1850/2000 Cost: 0.4742\n",
      "Epoch: 1860/2000 Cost: 0.4736\n",
      "Epoch: 1870/2000 Cost: 0.4729\n",
      "Epoch: 1880/2000 Cost: 0.4723\n",
      "Epoch: 1890/2000 Cost: 0.4717\n",
      "Epoch: 1900/2000 Cost: 0.4710\n",
      "Epoch: 1910/2000 Cost: 0.4704\n",
      "Epoch: 1920/2000 Cost: 0.4698\n",
      "Epoch: 1930/2000 Cost: 0.4692\n",
      "Epoch: 1940/2000 Cost: 0.4686\n",
      "Epoch: 1950/2000 Cost: 0.4680\n",
      "Epoch: 1960/2000 Cost: 0.4674\n",
      "Epoch: 1970/2000 Cost: 0.4668\n",
      "Epoch: 1980/2000 Cost: 0.4662\n",
      "Epoch: 1990/2000 Cost: 0.4656\n",
      "Epoch: 2000/2000 Cost: 0.4650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# Model Initialization\n",
    "model = nn.Linear(4,3)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epoch = 2000\n",
    "for epoch in range(nb_epoch+1):\n",
    "    \n",
    "    #H(x)\n",
    "    z = model(x_train)\n",
    "    \n",
    "    #cost\n",
    "    cost = F.cross_entropy(z,y_train)\n",
    "    \n",
    "    #update\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}/{} Cost: {:.4f}'.format(epoch, nb_epoch, cost.item()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-terror",
   "metadata": {},
   "source": [
    "### Softmax 회귀 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "little-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Cost: 1.6168\n",
      "Epoch: 100/1000 Cost: 0.6589\n",
      "Epoch: 200/1000 Cost: 0.5734\n",
      "Epoch: 300/1000 Cost: 0.5182\n",
      "Epoch: 400/1000 Cost: 0.4733\n",
      "Epoch: 500/1000 Cost: 0.4335\n",
      "Epoch: 600/1000 Cost: 0.3966\n",
      "Epoch: 700/1000 Cost: 0.3609\n",
      "Epoch: 800/1000 Cost: 0.3254\n",
      "Epoch: 900/1000 Cost: 0.2892\n",
      "Epoch: 1000/1000 Cost: 0.2541\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class Softmax_classification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Softmax_classification()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    #H(x)\n",
    "    z = model(x_train)\n",
    "    \n",
    "    #cost\n",
    "    cost = F.cross_entropy(z,y_train)\n",
    "    \n",
    "    #update\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: {}/{} Cost: {:.4f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-display",
   "metadata": {},
   "source": [
    "## Softmax regression으로 MNIST Data 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "juvenile-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cuda\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "casual-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "intellectual-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "informal-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "90.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n",
      "C:\\Users\\ImedisynRnD2\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#MNIST classifier 구현하기\n",
    "\n",
    "#dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                        train=True,\n",
    "                        transform=transforms.ToTensor(),\n",
    "                        download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                        train = False,\n",
    "                        transform=transforms.ToTensor(),\n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "driving-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.536015809\n",
      "Epoch: 0002 cost = 0.359202534\n",
      "Epoch: 0003 cost = 0.331243694\n",
      "Epoch: 0004 cost = 0.316479772\n",
      "Epoch: 0005 cost = 0.306780636\n",
      "Epoch: 0006 cost = 0.300162762\n",
      "Epoch: 0007 cost = 0.295002848\n",
      "Epoch: 0008 cost = 0.290735900\n",
      "Epoch: 0009 cost = 0.287426829\n",
      "Epoch: 0010 cost = 0.284311414\n",
      "Epoch: 0011 cost = 0.281867415\n",
      "Epoch: 0012 cost = 0.279607654\n",
      "Epoch: 0013 cost = 0.277803063\n",
      "Epoch: 0014 cost = 0.276044399\n",
      "Epoch: 0015 cost = 0.274502218\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# Dataset loader\n",
    "data_loader = DataLoader(dataset=mnist_train,\n",
    "                        batch_size = batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True)\n",
    "\n",
    "#MNIST data image of shape : 28 * 28 = 784\n",
    "linear = nn.Linear(784,10, bias = True).to(device)\n",
    "\n",
    "#cost and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        \n",
    "        #배치크기가 100이므로 아래의 연산에서 X는 (100,784)의 텐서가 된다.\n",
    "        X = X.view(-1,28*28).to(device)\n",
    "        \n",
    "        # 레이블은 one-hot encoding이 된 상태가 아니라 0~9의 정수.\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis,Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch +1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
